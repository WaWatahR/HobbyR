library(reshape2)
library(foreach)
library(doParallel)
library(tensorflow)
#dbcon = odbcDriverConnect('driver={SQL Server};server=localhost;database=ImageDatabase', case="nochange")
filepath = "D:/train/Type_"
cl = makeCluster(4)
registerDoParallel(cl)
#Initialize tensorflow
source("./config/tensorflow_init.R")
TransposeImportImages = function(filepath, xdim, ydim){
#Get a list of all images in filepath
image_names = dir(filepath, pattern=".jpg")
image_class = rep(0,3)
image_class[as.integer(str_sub(filepath,-1))] = 1
#subset for test
image_names = image_names[1:20]
#Import in parralel
images = do.call("rbind",foreach(i = 1:length(image_names), .packages = c('imager', 'stringr', 'reshape2')) %dopar% {
im = load.image(paste0(filepath, "/", image_names[i])) %>%
resize(xdim, ydim) %>%
grayscale
#temp = cbind(t(d$value), Name = image_names[i], Class = image_class)
temp = data.frame(t(as.data.frame(im)$value))
temp
})
#Save a list of one hot encoding for classs
class = do.call("rbind",foreach(i = 1:length(image_names), .packages = c('imager', 'stringr', 'reshape2')) %dopar% {
image_class
})
return(list(images, class))
}
input = list()
input[[1]] = data.frame()
input[[2]] = data.frame()
for(i in 1:3){
result = TransposeImportImages(paste0(filepath,i), xdim, ydim)
input[[1]] = rbind(input[[1]],result[[1]])
input[[2]] = rbind(input[[2]],result[[2]])
}
#Initialize tensorflow
ClassCount= 10L
xdim = 64L
ydim = 64L
PixelCount = xdim * ydim
sess <- tf$InteractiveSession()
optimizer <- tf$train$GradientDescentOptimizer(0.5)
x <- tf$placeholder(tf$float32, shape(NULL, PixelCount))
y_ <- tf$placeholder(tf$float32, shape(NULL, ClassCount))
b <- tf$Variable(tf$zeros(shape(10L)))
sess$run(tf$global_variables_initializer())
weight_variable <- function(shape) {
initial <- tf$truncated_normal(shape, stddev=0.1)
tf$Variable(initial)
}
bias_variable <- function(shape) {
initial <- tf$constant(0.1, shape=shape)
tf$Variable(initial)
}
conv2d <- function(x, W) {
tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')
}
max_pool_2x2 <- function(x) {
tf$nn$max_pool(x, ksize=c(1L, 2L, 2L, 1L), strides=c(1L, 2L, 2L, 1L), padding='SAME')
}
28*25
28*28
28*28/25
stride = 5L
ceiling(PixelCount / (stride^2))
FilterCount = ceiling(PixelCount / (stride^2))
64*64/25
ClassCount= 10L
xdim = 64L
ydim = 64L
Convstride = 5L
Poolstride = 2L
PixelCount = xdim * ydim
FilterCount = ceiling(PixelCount / (Convstride^2))
FilterCount
ceiling(FilterCount / (Convstride^2))
ceiling((FilterCount^2) / (Convstride^2))
ceiling((64*64) / (Convstride^2))
ceiling((16*16) / (Convstride^2))
ceiling((16*32) / (Convstride^2))
ceiling((32*32) / (Convstride^2))
1024/64
FilterCount = ceiling(PixelCount / (Convstride^2))
xdim = 32L
ydim = 32L
227*227*3
W=227*227*3
W=227
F=11
(W-F)/4+1
W=28
(W-F)/4+1
(W-F)/5+1
(W-F)/6+1
(W-F)/7+1
(W-F)
(W-F)/1+!
(W-F)/1+!1
(W-F)/1+1
(W-F)/2+1
F
F=5
(W-F)/1+1
W=7
F=3
S=1
P=
0
P=0
(W-F+2P)/S+1
(W-F+2*P)/S+1
W=28
F=5
S=1
P=0
(W-F+2*P)/S+1
28/5
28-5/2
28-5/1
28*28
/25
28*28/25
(28*28+1)/25
28*28/25
W=16
28
(16-5)
(16-5)^2
(16-5+1)^2
(28-5+1)^2
24*24
datasets <- tf$contrib$learn$datasets
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
stopCluster()
stopCluster(cl)
library(imager)
library(RODBC)
library(stringr)
library(reshape2)
library(foreach)
library(doParallel)
library(tensorflow)
#dbcon = odbcDriverConnect('driver={SQL Server};server=localhost;database=ImageDatabase', case="nochange")
filepath = "D:/train/Type_"
cl = makeCluster(4)
registerDoParallel(cl)
#Initialize tensorflow
source("./config/tensorflow_init.R")
#Initialize tensorflow
ClassCount= 10L
xdim = 64L
ydim = 64L
Convstride = 5L
Poolstride = 2L
PixelCount = xdim * ydim
FilterCount = ceiling(PixelCount / (Convstride^2))
sess <- tf$InteractiveSession()
optimizer <- tf$train$GradientDescentOptimizer(0.5)
x <- tf$placeholder(tf$float32, shape(NULL, PixelCount))
y_ <- tf$placeholder(tf$float32, shape(NULL, ClassCount))
b <- tf$Variable(tf$zeros(shape(10L)))
sess$run(tf$global_variables_initializer())
weight_variable <- function(shape) {
initial <- tf$truncated_normal(shape, stddev=0.1)
tf$Variable(initial)
}
bias_variable <- function(shape) {
initial <- tf$constant(0.1, shape=shape)
tf$Variable(initial)
}
conv2d <- function(x, W) {
tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')
}
max_pool_2x2 <- function(x) {
tf$nn$max_pool(x, ksize=c(1L, 2L, 2L, 1L), strides=c(1L, 2L, 2L, 1L), padding='SAME')
}
#Initialize tensorflow
ClassCount= 3L
xdim = 64L
ydim = 64L
Convstride = 5L
Poolstride = 2L
PixelCount = xdim * ydim
FilterCount = ceiling(PixelCount / (Convstride^2))
sess <- tf$InteractiveSession()
optimizer <- tf$train$GradientDescentOptimizer(0.5)
x <- tf$placeholder(tf$float32, shape(NULL, PixelCount))
y_ <- tf$placeholder(tf$float32, shape(NULL, ClassCount))
b <- tf$Variable(tf$zeros(shape(10L)))
sess$run(tf$global_variables_initializer())
weight_variable <- function(shape) {
initial <- tf$truncated_normal(shape, stddev=0.1)
tf$Variable(initial)
}
bias_variable <- function(shape) {
initial <- tf$constant(0.1, shape=shape)
tf$Variable(initial)
}
conv2d <- function(x, W) {
tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')
}
max_pool_2x2 <- function(x) {
tf$nn$max_pool(x, ksize=c(1L, 2L, 2L, 1L), strides=c(1L, 2L, 2L, 1L), padding='SAME')
}
#Initialize tensorflow
ClassCount= 3L
xdim = 64L
ydim = 64L
Convstride = 5L
Poolstride = 2L
PixelCount = xdim * ydim
FilterCount = ceiling(PixelCount / (Convstride^2))
sess <- tf$InteractiveSession()
optimizer <- tf$train$GradientDescentOptimizer(0.5)
x <- tf$placeholder(tf$float32, shape(NULL, PixelCount))
y_ <- tf$placeholder(tf$float32, shape(NULL, ClassCount))
b <- tf$Variable(tf$zeros(shape(10L)))
sess$run(tf$global_variables_initializer())
weight_variable <- function(shape) {
initial <- tf$truncated_normal(shape, stddev=0.1)
tf$Variable(initial)
}
bias_variable <- function(shape) {
initial <- tf$constant(0.1, shape=shape)
tf$Variable(initial)
}
conv2d <- function(x, W) {
tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')
}
max_pool_2x2 <- function(x) {
tf$nn$max_pool(x, ksize=c(1L, 2L, 2L, 1L), strides=c(1L, 2L, 2L, 1L), padding='SAME')
}
W_conv1 <- weight_variable(shape(Convstride, Convstride, 1L, FilterCount))
b_conv1 <- bias_variable(shape(32L))
x_image <- tf$reshape(x, shape(-1L, xdim, ydim, 1L))
h_conv1 <- tf$nn$relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 <- max_pool_2x2(h_conv1)
X
#Initialize tensorflow
ClassCount= 3L
xdim = 64L
ydim = 64L
Convstride = 5L
Poolstride = 2L
PixelCount = xdim * ydim
FilterCount = ceiling(PixelCount / (Convstride^2))
sess <- tf$InteractiveSession()
optimizer <- tf$train$GradientDescentOptimizer(0.5)
x <- tf$placeholder(tf$float32, shape(NULL, PixelCount))
y_ <- tf$placeholder(tf$float32, shape(NULL, ClassCount))
b <- tf$Variable(tf$zeros(shape(10L)))
sess$run(tf$global_variables_initializer())
weight_variable <- function(shape) {
initial <- tf$truncated_normal(shape, stddev=0.1)
tf$Variable(initial)
}
bias_variable <- function(shape) {
initial <- tf$constant(0.1, shape=shape)
tf$Variable(initial)
}
conv2d <- function(x, W) {
tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')
}
max_pool_2x2 <- function(x) {
tf$nn$max_pool(x, ksize=c(1L, 2L, 2L, 1L), strides=c(1L, 2L, 2L, 1L), padding='SAME')
}
X
x
4096/32
W_conv1 <- weight_variable(shape(5L, 5L, 1L, 32L))
b_conv1 <- bias_variable(shape(32L))
x_image <- tf$reshape(x, shape(-1L, xdim, ydim, 1L))
h_conv1 <- tf$nn$relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 <- max_pool_2x2(h_conv1)
W_conv2 <- weight_variable(shape = shape(5L, 5L, 32L, 64L))
b_conv2 <- bias_variable(shape = shape(64L))
h_conv2 <- tf$nn$relu(conv2d(h_pool1, W_conv2) + b_conv2)
h_pool2 <- max_pool_2x2(h_conv2)
W_fc1 <- weight_variable(shape(7L * 7L * 64L, 1024L))
b_fc1 <- bias_variable(shape(1024L))
h_pool2_flat <- tf$reshape(h_pool2, shape(-1L, 7L * 7L * 64L))
h_fc1 <- tf$nn$relu(tf$matmul(h_pool2_flat, W_fc1) + b_fc1)
keep_prob <- tf$placeholder(tf$float32)
h_fc1_drop <- tf$nn$dropout(h_fc1, keep_prob)
W_fc2 <- weight_variable(shape(1024L, ClassCount))
b_fc2 <- bias_variable(shape(ClassCount))
y_conv <- tf$nn$softmax(tf$matmul(h_fc1_drop, W_fc2) + b_fc2)
cross_entropy <- tf$reduce_mean(-tf$reduce_sum(y_ * tf$log(y_conv), reduction_indices=1L))
train_step <- tf$train$AdamOptimizer(1e-4)$minimize(cross_entropy)
correct_prediction <- tf$equal(tf$argmax(y_conv, 1L), tf$argmax(y_, 1L))
accuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))
prediction <- tf$argmax(y_conv, 1L)
probabilities <- y_conv
sess$run(tf$global_variables_initializer())
train_step$run(feed_dict = dict(x = as.matrix(input[[1]]), y_ = as.matrix(input[[2]]), keep_prob = 0.5))
input
input[[1]][1,]
input = list()
input[[1]] = data.frame()
input[[2]] = data.frame()
for(i in 1:3){
result = TransposeImportImages(paste0(filepath,i), xdim, ydim)
input[[1]] = rbind(input[[1]],result[[1]])
input[[2]] = rbind(input[[2]],result[[2]])
}
train_step$run(feed_dict = dict(x = as.matrix(input[[1]]), y_ = as.matrix(input[[2]]), keep_prob = 0.5))
7*7*64
983040^0.5
983040^3200
983040/3200
983040/3201
983040/3202
983040/320
983040/4020
983040/4120
983040/4220
983040/4220
983040/4320
(64-5)/1+1
(24-5)/1+1
(28-5)/1+1
(24-2)/2+1
(14-2)/2+1
(60-2)/2+1
(30-2)/2+1
W_conv1 <- weight_variable(shape(Convstride, Convstride, 1L, FilterCount))
b_conv1 <- bias_variable(shape(32L))
x_image <- tf$reshape(x, shape(-1L, xdim, ydim, 1L))
h_conv1 <- tf$nn$relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 <- max_pool_2x2(h_conv1)
#second layer
W_conv2 <- weight_variable(shape = shape(Convstride, Convstride, FilterCount, 64L))
b_conv2 <- bias_variable(shape = shape(64L))
h_conv2 <- tf$nn$relu(conv2d(h_pool1, W_conv2) + b_conv2)
h_pool2 <- max_pool_2x2(h_conv2)
#Densely connected layer
W_fc1 <- weight_variable(shape(15L * 15L * 64L, 1024L))
b_fc1 <- bias_variable(shape(1024L))
h_pool2_flat <- tf$reshape(h_pool2, shape(-1L, 15L * 15L * 64L))
h_fc1 <- tf$nn$relu(tf$matmul(h_pool2_flat, W_fc1) + b_fc1)
#drop out layer for overfitting
keep_prob <- tf$placeholder(tf$float32)
h_fc1_drop <- tf$nn$dropout(h_fc1, keep_prob)
#Read out layer
W_fc2 <- weight_variable(shape(1024L, ClassCount))
b_fc2 <- bias_variable(shape(ClassCount))
y_conv <- tf$nn$softmax(tf$matmul(h_fc1_drop, W_fc2) + b_fc2)
#Once all layers defined
cross_entropy <- tf$reduce_mean(-tf$reduce_sum(y_ * tf$log(y_conv), reduction_indices=1L))
train_step <- tf$train$AdamOptimizer(1e-4)$minimize(cross_entropy)
correct_prediction <- tf$equal(tf$argmax(y_conv, 1L), tf$argmax(y_, 1L))
accuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))
prediction <- tf$argmax(y_conv, 1L)
probabilities <- y_conv
sess$run(tf$global_variables_initializer())
train_step$run(feed_dict = dict(x = as.matrix(input[[1]]), y_ = as.matrix(input[[2]]), keep_prob = 0.5))
15*15*64
W_conv1 <- weight_variable(shape(Convstride, Convstride, 1L, FilterCount))
b_conv1 <- bias_variable(shape(32L))
x_image <- tf$reshape(x, shape(-1L, xdim, ydim, 1L))
h_conv1 <- tf$nn$relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 <- max_pool_2x2(h_conv1)
#second layer
W_conv2 <- weight_variable(shape = shape(Convstride, Convstride, FilterCount, 64L))
b_conv2 <- bias_variable(shape = shape(64L))
h_conv2 <- tf$nn$relu(conv2d(h_pool1, W_conv2) + b_conv2)
h_pool2 <- max_pool_2x2(h_conv2)
#Densely connected layer
W_fc1 <- weight_variable(shape(15L * 15L * 64L, 1024L))
b_fc1 <- bias_variable(shape(1024L))
library(imager)
library(RODBC)
library(stringr)
library(reshape2)
library(foreach)
library(doParallel)
library(tensorflow)
#dbcon = odbcDriverConnect('driver={SQL Server};server=localhost;database=ImageDatabase', case="nochange")
filepath = "D:/train/Type_"
cl = makeCluster(4)
registerDoParallel(cl)
#Initialize tensorflow
source("./config/tensorflow_init.R")
#Initialize tensorflow
ClassCount= 3L
xdim = 64L
ydim = 64L
Convstride = 5L
Poolstride = 2L
PixelCount = xdim * ydim
FilterCount = ceiling(PixelCount / (Convstride^2))
sess <- tf$InteractiveSession()
optimizer <- tf$train$GradientDescentOptimizer(0.5)
x <- tf$placeholder(tf$float32, shape(NULL, PixelCount))
y_ <- tf$placeholder(tf$float32, shape(NULL, ClassCount))
b <- tf$Variable(tf$zeros(shape(10L)))
sess$run(tf$global_variables_initializer())
weight_variable <- function(shape) {
initial <- tf$truncated_normal(shape, stddev=0.1)
tf$Variable(initial)
}
bias_variable <- function(shape) {
initial <- tf$constant(0.1, shape=shape)
tf$Variable(initial)
}
conv2d <- function(x, W) {
tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')
}
max_pool_2x2 <- function(x) {
tf$nn$max_pool(x, ksize=c(1L, 2L, 2L, 1L), strides=c(1L, 2L, 2L, 1L), padding='SAME')
}
W_conv1 <- weight_variable(shape(Convstride, Convstride, 1L, FilterCount))
b_conv1 <- bias_variable(shape(32L))
x_image <- tf$reshape(x, shape(-1L, xdim, ydim, 1L))
h_conv1 <- tf$nn$relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 <- max_pool_2x2(h_conv1)
#second layer
W_conv2 <- weight_variable(shape = shape(Convstride, Convstride, FilterCount, 64L))
b_conv2 <- bias_variable(shape = shape(64L))
h_conv2 <- tf$nn$relu(conv2d(h_pool1, W_conv2) + b_conv2)
h_pool2 <- max_pool_2x2(h_conv2)
tf=NULL
tf
library(imager)
library(RODBC)
library(stringr)
library(reshape2)
library(foreach)
library(doParallel)
library(tensorflow)
#dbcon = odbcDriverConnect('driver={SQL Server};server=localhost;database=ImageDatabase', case="nochange")
filepath = "D:/train/Type_"
cl = makeCluster(4)
registerDoParallel(cl)
#Initialize tensorflow
source("./config/tensorflow_init.R")
#Initialize tensorflow
ClassCount= 3L
xdim = 64L
ydim = 64L
Convstride = 5L
Poolstride = 2L
PixelCount = xdim * ydim
FilterCount = ceiling(PixelCount / (Convstride^2))
sess <- tf$InteractiveSession()
optimizer <- tf$train$GradientDescentOptimizer(0.5)
x <- tf$placeholder(tf$float32, shape(NULL, PixelCount))
y_ <- tf$placeholder(tf$float32, shape(NULL, ClassCount))
b <- tf$Variable(tf$zeros(shape(10L)))
sess$run(tf$global_variables_initializer())
weight_variable <- function(shape) {
initial <- tf$truncated_normal(shape, stddev=0.1)
tf$Variable(initial)
}
bias_variable <- function(shape) {
initial <- tf$constant(0.1, shape=shape)
tf$Variable(initial)
}
conv2d <- function(x, W) {
tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')
}
max_pool_2x2 <- function(x) {
tf$nn$max_pool(x, ksize=c(1L, 2L, 2L, 1L), strides=c(1L, 2L, 2L, 1L), padding='SAME')
}
7*7*64
7*7*64/1024
64*64
64*64/1024
3136/1024
(3136-2)/2+1
train_step$run(feed_dict = dict(x = as.matrix(input[[1]]), y_ = as.matrix(input[[2]]), keep_prob = 0.5))
W_conv1 <- weight_variable(shape(5L, 5L, 1L, 32L))
b_conv1 <- bias_variable(shape(32L))
x_image <- tf$reshape(x, shape(-1L, xdim, ydim, 1L))
h_conv1 <- tf$nn$relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 <- max_pool_2x2(h_conv1)
#second layer
W_conv2 <- weight_variable(shape = shape(5L, 5L, 32L, 64L))
b_conv2 <- bias_variable(shape = shape(64L))
#Initialize tensorflow
ClassCount= 3L
xdim = 64L
ydim = 64L
Convstride = 5L
Poolstride = 2L
PixelCount = xdim * ydim
FilterCount = ceiling(PixelCount / (Convstride^2))
sess <- tf$InteractiveSession()
optimizer <- tf$train$GradientDescentOptimizer(0.5)
x <- tf$placeholder(tf$float32, shape(NULL, PixelCount))
y_ <- tf$placeholder(tf$float32, shape(NULL, ClassCount))
b <- tf$Variable(tf$zeros(shape(10L)))
sess$run(tf$global_variables_initializer())
weight_variable <- function(shape) {
initial <- tf$truncated_normal(shape, stddev=0.1)
tf$Variable(initial)
}
bias_variable <- function(shape) {
initial <- tf$constant(0.1, shape=shape)
tf$Variable(initial)
}
conv2d <- function(x, W) {
tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')
}
max_pool_2x2 <- function(x) {
tf$nn$max_pool(x, ksize=c(1L, 2L, 2L, 1L), strides=c(1L, 2L, 2L, 1L), padding='SAME')
}
